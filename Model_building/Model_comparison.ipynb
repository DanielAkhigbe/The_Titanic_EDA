{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Modules\n",
    "For this part of the model building process, the modules to be imported would be \"joblib\",\n",
    "\n",
    "\"sklearn\", \"pandas\" and \"time\". The \"joblib\" module would be used to load the saved machine learning models.\n",
    "\n",
    "The \"sklearn\" module would now be used to test the accuracy, precision and recall of the selected models\n",
    "\n",
    "in order to determine the best option. The \"time\" module would be used to calculate the speed/latency of the \n",
    "\n",
    "models when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data\n",
    "The validation and test set's features and labels would be used to further evaluate the model\n",
    "\n",
    "and also test its performance on unseen data.\n",
    "\n",
    "They are brought in using the pandas \"read_csv\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv(r'...\\...\\...\\titanic_EDA\\Split_data\\validation_features.csv')\n",
    "\n",
    "validation_labels = pd.read_csv(r'...\\...\\...\\titanic_EDA\\Split_data\\validation_labels.csv')\n",
    "\n",
    "test_features = pd.read_csv(r'...\\...\\...\\titanic_EDA\\Split_data\\test_features.csv')\n",
    "\n",
    "test_labels = pd.read_csv(r'...\\...\\...\\titanic_EDA\\Split_data\\test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Models\n",
    "The Logistic Regression and Random Forest models already created and saved previosuly would be\n",
    "\n",
    "loaded into the Notebook with the stored hyperparameter settings using \"joblib.load()\" \n",
    "\n",
    "and placed in a dictionary as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for model in ['LR', 'RF']:\n",
    "    models[model] = joblib.load(r'...\\...\\...\\titanic_EDA\\Models\\{}_model.pkl'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=10),\n",
       " 'RF': RandomForestClassifier(max_depth=4, n_estimators=50)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "The function below was created to assess the models and calculate their accuracy, precision, recall and speed.\n",
    "\n",
    "It would also print the results in a nice readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    start = time()\n",
    "    prediction = model.predict(features)\n",
    "    end = time()\n",
    "    accuracy = round(accuracy_score(labels, prediction), 3)\n",
    "    precision = round(precision_score(labels, prediction), 3)\n",
    "    recall = round(recall_score(labels, prediction), 3)\n",
    "    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name, accuracy, precision, recall, \n",
    "                                                                                    round((end - start)*1000, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Validation Set\n",
    "The \"evaluate_model\" function created above would now be used to judge the performance of the selected models\n",
    "\n",
    "on the validation set's features and labels.\n",
    "\n",
    "From the output it is clear that the Random Forest model performs better in terms of accuracy and precision,\n",
    "\n",
    "however, it takes much longer than the Logistic Regression model to make predictions.\n",
    "\n",
    "Since there is no real time constraint in this case, it is safe to say that the Random Forest model\n",
    "\n",
    "is the better option and so this would be chosen to evaluate the Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.787 / Precision: 0.654 / Recall: 0.63 / Latency: 3.0ms\n",
      "RF -- Accuracy: 0.792 / Precision: 0.667 / Recall: 0.63 / Latency: 11.0ms\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    evaluate_model(name, model, validation_features, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Test Set\n",
    "Now that the model with the best performance on the Validation set has been chosen, it is time to \n",
    "\n",
    "see how well it does on the Test set's features and labels. The \"test_features\" would be the input\n",
    "\n",
    "that the model would make predictions on. Those predictions would be compared with the actual outcomes\n",
    "\n",
    "in the \"test_labels\" and that will tell how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest -- Accuracy: 0.799 / Precision: 0.87 / Recall: 0.618 / Latency: 11.0ms\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Random Forest', models['RF'], test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
